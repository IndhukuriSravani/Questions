# main.py

import pandas as pd
import random
import string
import re
import pickle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# Step 1: Generate Dummy Dataset (1000 Fake + 1000 Real)
fake_keywords = ["Shocking!", "Clickbait alert", "Scandal revealed", "Unbelievable!", "You won't believe this"]
real_keywords = ["Government policy update", "Scientific breakthrough", "Economic growth report", "Election results", "Technology advancement"]

data = []

for _ in range(1000):
    data.append({
        "text": f"{random.choice(fake_keywords)} This article claims something unexpected.",
        "label": 1
    })

for _ in range(1000):
    data.append({
        "text": f"{random.choice(real_keywords)} reported in today's headlines.",
        "label": 0
    })

random.shuffle(data)
df = pd.DataFrame(data)
df.to_csv("fake_news_dummy_dataset.csv", index=False)

# Step 2: Preprocess text
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\\S+|www\\S+|https\\S+", '', text)
    text = re.sub(r'\@w+|\#','', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    stop_words = stopwords.words('english')
    text = " ".join([word for word in text.split() if word not in stop_words])
    return text

df['text'] = df['text'].apply(clean_text)

# Step 3: Train model
X = df['text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Pipeline([
    ('tfidf', TfidfVectorizer(max_df=0.7)),
    ('nb', MultinomialNB())
])

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f"✅ Model trained with accuracy: {accuracy_score(y_test, y_pred):.2f}")

# Step 4: Save model
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)

print("✅ Model saved to model.pkl")
